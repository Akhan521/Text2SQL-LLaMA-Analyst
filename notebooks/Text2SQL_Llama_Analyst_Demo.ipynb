{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8898b61227f489bb34879fd43124c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_107db926353a45faa784c8c28bebd0a4",
              "IPY_MODEL_a938be5b086a4c2fba9bdabc614b3cc3",
              "IPY_MODEL_7d671a2e5298491494bdf78b7fb90e17"
            ],
            "layout": "IPY_MODEL_bc8ea1b5ab7c434bb2234325138feb56"
          }
        },
        "107db926353a45faa784c8c28bebd0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_135daceab762476ea0315eb5e9722e31",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c315000738a5400d86ed587946f85f02",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "a938be5b086a4c2fba9bdabc614b3cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_777a418a3d33431197ba76229e56c93c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_930b0bb9a3be42a3b0c6eb289dd5a26d",
            "value": 2
          }
        },
        "7d671a2e5298491494bdf78b7fb90e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9639019e80243faad1749a248e562b3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_37d5691590ac40f29a609d2614864130",
            "value": "‚Äá2/2‚Äá[00:09&lt;00:00,‚Äá‚Äá4.11s/it]"
          }
        },
        "bc8ea1b5ab7c434bb2234325138feb56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "135daceab762476ea0315eb5e9722e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c315000738a5400d86ed587946f85f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "777a418a3d33431197ba76229e56c93c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930b0bb9a3be42a3b0c6eb289dd5a26d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9639019e80243faad1749a248e562b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d5691590ac40f29a609d2614864130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶ô Text2SQL Llama Analyst Demo\n",
        "\n",
        "üëãüèº Welcome to my interactive demo which showcases my fine-tuned `LLaMA-2-7B` model that translates natural language questions into SQL queries.\n",
        "\n",
        "It‚Äôs a fun and practical example of how large language models can be adapted to domain-specific tasks using lightweight fine-tuning techniques like LoRA and quantization.\n",
        "\n",
        "I really hope you enjoy my demo, learning about my project, and seeing the reflections I've made along the way!\n",
        "\n",
        "## üí° What This Project Does\n",
        "\n",
        "- **Task**: Converts natural language questions like \"What are the names of all customers who placed an order in 2025?\" into structured SQL queries.\n",
        "- **Model**: `LLaMA-2-7B`, fine-tuned using parameter-efficient fine-tuning techniques: [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685) and Quantization (Explained in detail below.)\n",
        "- **Dataset**: [ChrisHayduk/Llama-2-SQL-Dataset](https://huggingface.co/datasets/ChrisHayduk/Llama-2-SQL-Dataset), which is a curated dataset of natural language-to-SQL examples.\n",
        "\n",
        "## üí≠ Why This Matters\n",
        "\n",
        "- **Large Language Models (LLMs)** are great at performing general tasks, but they aren't specialists.\n",
        "  > Fine-tuning enables them to perform highly specific tasks with greater accuracy and consistency.\n",
        "- My project demonstrates how to **align a foundation model with user intent** through **parameter-efficient fine-tuning**.\n",
        "- You‚Äôll see how a base LLaMA model can become a **task-specific assistant** for querying structured databases.\n",
        "- Additionally, you'll be able to **compare my model's SQL predictions against ground truth** examples from the evaluation set, which is a great way to explore and see my model's strengths and limitations first-hand.\n",
        "\n"
      ],
      "metadata": {
        "id": "9CoB1xEFAKie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß A Deeper Look into LoRA + Quantization\n",
        "\n",
        "My project uses two modern techniques to make training large models like LLaMA-2 efficient and accessible, even on limited hardware.\n",
        "\n",
        "### üëâüèª LoRA (Low-Rank Adaptation)\n",
        "\n",
        "- Instead of updating all 7 billion parameters in the base LLaMA model, **LoRA freezes the original weights** and only targets/retrains specific modules like the attention and feedforward layers of the model. We move away from full-fine-tuning and only focus our attention on updating a fraction of our parameters (those relevant to our task).\n",
        "- This dramatically reduces memory and compute needs, while still allowing the model to **learn new tasks like Text-to-SQL**.\n",
        "\n",
        "### üëâüèª 4-bit Quantization\n",
        "\n",
        "- Normally, LLaMA-2 uses 16-bit or 32-bit weights which is memory-heavy.\n",
        "- **Quantization shrinks the model weights to just 4 bits**, making them **smaller and faster to load**, while still retaining performance.\n",
        "- This allows us to **run the model on limited GPUs or even CPUs** for inference.\n",
        "\n",
        "\n",
        "Together, **LoRA + Quantization (QLoRA)** allow us to fine-tune and deploy a powerful language model using minimal resources (making cutting-edge AI more accessible and efficient).\n"
      ],
      "metadata": {
        "id": "qPP6rRYgDyXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üë®‚Äçüíª Try It Yourself\n",
        "\n",
        "Follow the cells below to:\n",
        "- Setup my demo code\n",
        "- Enter your own natural language questions and get corresponding SQL outputs\n",
        "- Explore sample examples from the evaluation set and compare my model‚Äôs SQL predictions to the correct/expected SQL outputs"
      ],
      "metadata": {
        "id": "9zbS0N6ZGoYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''SETUP: Cloning my Repo and Loading Our Fine-Tuned Model'''\n",
        "\n",
        "# Clone my Repo\n",
        "!git clone https://github.com/Akhan521/Text2SQL-LLaMA-Analyst.git\n",
        "%cd Text2SQL-LLaMA-Analyst\n",
        "\n",
        "# Install Dependencies\n",
        "!pip install datasets\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n"
      ],
      "metadata": {
        "id": "ZyHtZorBqqeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Loading our Base Model (Necessary for Loading our Fine-Tuned Model)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Base Model (Llama-2-7b-hf from Hugging Face)\n",
        "base_model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "c8898b61227f489bb34879fd43124c91",
            "107db926353a45faa784c8c28bebd0a4",
            "a938be5b086a4c2fba9bdabc614b3cc3",
            "7d671a2e5298491494bdf78b7fb90e17",
            "bc8ea1b5ab7c434bb2234325138feb56",
            "135daceab762476ea0315eb5e9722e31",
            "c315000738a5400d86ed587946f85f02",
            "777a418a3d33431197ba76229e56c93c",
            "930b0bb9a3be42a3b0c6eb289dd5a26d",
            "a9639019e80243faad1749a248e562b3",
            "37d5691590ac40f29a609d2614864130"
          ]
        },
        "collapsed": true,
        "id": "oyKIPtZjF_AB",
        "outputId": "54dafdd4-7429-4d1c-b718-937b531e2f6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8898b61227f489bb34879fd43124c91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Loading our Fine-Tuned Model (On Top of our Base Model)\n",
        "from peft import PeftModel\n",
        "\n",
        "# Note: Our Fine-Tuned Model is a LoRA Adaptor.\n",
        "finetuned_model_name = \"akhan365/llama2-finetuned-for-text2sql\"\n",
        "finetuned_model = PeftModel.from_pretrained(model, finetuned_model_name)\n",
        "\n",
        "# 3. Loading our Tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "print(\"‚úÖ Fine-tuned model loaded and ready for inference!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boge8_VhGJum",
        "outputId": "c82f5fca-cc39-4586-940c-50de5c8be924"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fine-tuned model loaded and ready for inference!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.generate import generate\n",
        "\n",
        "def generate_query(prompt: str):\n",
        "\n",
        "    print(\"\\nü§ñ Fine-Tuned Llama-2 Response:\")\n",
        "    print(\"=\" * 50)\n",
        "    finetuned_output = generate(prompt, model, tokenizer, device=device)\n",
        "\n",
        "    print(finetuned_output)"
      ],
      "metadata": {
        "id": "RecwhfTHsD3-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play around with the prompt:\n",
        "\n",
        "'''\n",
        "We'll go ahead and compare our models using a sample question from our evaluation set.\n",
        "We'll also provide the expected solution for the given sample question.\n",
        "'''\n",
        "from src.data_loader import load_data\n",
        "\n",
        "_, eval_dataset = load_data(dataset_name = \"ChrisHayduk/Llama-2-SQL-Dataset\")\n",
        "\n",
        "# If you modify this prompt, make sure to re-run this code cell!\n",
        "chosen_sample = torch.randint(0, len(eval_dataset), ()).item() # A random sample question from our eval. dataset.\n",
        "sample_question = eval_dataset[chosen_sample]['input']\n",
        "correct_answer  = eval_dataset[chosen_sample]['output']\n",
        "\n",
        "generate_query(sample_question)\n",
        "\n",
        "print(\"\\nüóùÔ∏è Expected Answer:\")\n",
        "print(\"=\" * 50)\n",
        "print(correct_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ciIqjKLta4y",
        "outputId": "d2109bfd-b35c-4a1d-97db-2c1231975f16"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ Fine-Tuned Llama-2 Response:\n",
            "==================================================\n",
            "Below is an instruction that describes a SQL generation task, paired with an input that provides further context about the available table schemas. Write SQL code that appropriately answers the request.\n",
            "\n",
            "### Instruction:\n",
            "Which TV Station has a Romaji Title of kegareta shita?\n",
            "\n",
            "### Input:\n",
            "CREATE TABLE table_name_43 (tv_station VARCHAR, romaji_title VARCHAR)\n",
            "\n",
            "### Response:  SELECT tv station FROM Table Name WHERE romanji title = \"kega retta ShITa\"\n",
            "\n",
            "üóùÔ∏è Expected Answer:\n",
            "==================================================\n",
            "SELECT tv_station FROM table_name_43 WHERE romaji_title = \"kegareta shita\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí° Potential Improvements\n",
        "\n",
        "While my fine-tuned LLaMA-2 model shows meaningful improvements over the base model on translating natural language to SQL, there is still much room for growth:\n",
        "\n",
        "- üîç **Output Quality**: My model may produce incomplete or syntactically incorrect SQL queries, especially on complex questions.\n",
        "- üìä **Dataset Limitations**: The training dataset (`ChrisHayduk/Llama-2-SQL-Dataset`) is relatively small, which limits my model‚Äôs generalization ability.\n",
        "- üìè **Evaluation Metrics**: A more rigorous evaluation framework using string-match accuracy, execution accuracy, or SQL parse correctness would provide deeper insight into model performance.\n",
        "- ‚ùî **Question Understanding**: My model sometimes misinterprets ambiguous or vague natural language questions; Improving my model's ability to understand intent could lead to more accurate SQL generation.\n",
        "\n",
        "\n",
        "Despite these challenges, my model consistently **outperforms the base LLaMA-2** on this task, demonstrating how even lightweight fine-tuning with LoRA can yield domain-specific gains with minimal compute.\n"
      ],
      "metadata": {
        "id": "n9TU7vk4Zmhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚åõ Final Remarks\n",
        "\n",
        "My project demonstrates how even a massive open-source model like **LLaMA-2** can be adapted to a highly specific task like **Text-to-SQL Generation** with **low-rank adaptation (LoRA)** and **parameter-efficient fine-tuning**.\n",
        "\n",
        "Through this project, I gained experience in:\n",
        "\n",
        "- Fine-tuning large language models with **PEFT (LoRA)** techniques\n",
        "- Managing **quantization trade-offs** for faster training/inference\n",
        "- Designing clean, modular training and inference workflows\n",
        "- Publishing models to the **Hugging Face Hub**\n",
        "- Building a demo that's accessible to both technical and non-technical users\n",
        "\n",
        "While I believe there‚Äôs still much room for improvement, this project shows the real-world viability of domain adaptation using open-source models/weights.\n",
        "\n",
        "\n",
        "üìÅ **My Repository**: [Text2SQL-LLaMA-Analyst](https://github.com/Akhan521/Text2SQL-LLaMA-Analyst)  \n",
        "üåê **My Portfolio**: [https://aamir-khans-portfolio.vercel.app/](https://aamir-khans-portfolio.vercel.app/)  \n",
        "üîó **My LinkedIn**: [https://www.linkedin.com/in/aamir-khan-aak521/](https://www.linkedin.com/in/aamir-khan-aak521/)\n",
        "\n",
        "Thanks for checking out my demo! Feel free to explore my code, play around with my model, or reach out for fun conversations!\n"
      ],
      "metadata": {
        "id": "UVeFBiMrbzz6"
      }
    }
  ]
}